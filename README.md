# etl-scrapy-project

## About this project

<p>This project serves as portfolio and it showcases an <b>ETL</b> workflow, employing <b>Scrapy</b> to web scrape product listings from the initial ten pages of <a href='https://lista.mercadolivre.com.br/tenis-corrida-masculino'>Sports Shoes category on the Mercado Livre Website</a>.<br>

The scraped data is then transformed and persisted in an <b>SQLite</b> database.<br>

Following the above steps, a <b>Streamlit</b> dashboard for interactive data visualization is then built.</p>

---

## Overall Architecture

![architecture](images\architecture.png)

0. **Data Source:** https://lista.mercadolivre.com.br/tenis-corrida-masculino

1. **Data Ingestion**<br>
Scrap data from the website using **Scrapy**, limiting the scraping to 10 pages at max to avoid overloading the website with requests;

2. **Data Transformation**<br>
**Python** and **Pandas** are employed to guarantee that data is analysed, cleaned and converted to fit requirements and ensure that it is in a proper format for analysis and querying;

3. **Data Storage**<br>
The generated dataframes are loaded into a **SQLite** database for data manipulation;

4. **Data Visualization**<br>
**Streamlit** is employed to build relevant KPIs and graphs.

## Technologies Used

- **Collection**: Scrapy
- **Processing**: Python
- **Transformation**: Pandas
- **Storaging**: SQLite
- **Data Visualization**: Streamlit

## Running instructions

### Pre-requisites

1. Python version **3.12.1**
2. pyenv-win
3. Pandas
4. sqlite3
5. Streamlit

### Running the web scraping script

<p>Run the following code inside <b>src</b> folder:</p>

```bash
scrapy crawl mercadolivre -o ../../data/data.jsonl
```

<p>Run Pandas transformation code inside <b>transform</b> folder</p>

```bash
python transform/main.py
```

<p>Run Streamlit inside <b>dashboard</b> folder:</p>

```bash
streamlit run dashboard/app.py 
```

## Directory Structure

```plaintext
project/                   
├── data/                           # Files generated by Scrapy and database config
├── images/                         # README Images
├── src/                            # Python Source Code
│   ├── collection/                 # Scrapy files
│       ├── spiders/                # Crawler data
│           ├── mercadolivre.py     # Generated spider (crawler)
│   ├── dashboard/                  # Streamlit code for data visualization
│   ├── transform/                  # Pandas code for data treatment
│   ├── scrapy.cfg                  # Scrapy config file
├── .venv/                          # Virtual Environment Variables
└── README.md                       # Readme File
```

## Next Steps

1. Generate docstrings to document project with MkDocs;
2. Employ DuckDB as storage solution in place of SQLite.